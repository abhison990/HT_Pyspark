https://phoenixnap.com/kb/install-hadoop-ubuntu
https://phoenixnap.com/kb/install-hive-on-ubuntu
Access Hadoop UI from Browser
Use your preferred browser and navigate to your localhost URL or IP. The default port number 9870 gives you access to the Hadoop NameNode UI:

http://localhost:9870


Conclusion
You have successfully installed Hadoop on Ubuntu and deployed it in a pseudo-distributed mode. 
A single node Hadoop deployment is an excellent starting point to explore basic HDFS commands 
and acquire the experience you need to design a fully distributed Hadoop cluster.


 guava-14.0.1.jar

guava-27.0-jre.jar

FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: 
java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient

https://stackoverflow.com/questions/28695444/apache-hive-unable-to-instantiate-org-apache-hadoop-hive-metastore-hivemetasto

======================================================================================

wget https://downloads.apache.org/hive/hive-2.3.7/apache-hive-3.1.2-bin.tar.gz

/usr/lib/jvm/java-8-openjdk-amd64
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

export HIVE_HOME=/home/abhijeet/Downloads/apache-hive-2.3.7-bin
export PATH=$PATH:$HIVE_HOME/bin

# Allow alternate conf dir location.
HIVE_CONF_DIR="${HIVE_CONF_DIR:-$HIVE_HOME/conf}"

export HIVE_CONF_DIR=/home/abhijeet/Downloads/apache-hive-2.3.7-bin/conf
export HADOOP_HOME=/home/abhijeet/Downloads/hadoop-3.2.1
export HIVE_AUX_JARS_PATH=$HIVE_AUX_JARS_PATH


export SPARK_HOME=~/Downloads/spark-2.4.7-bin-hadoop2.7
export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
export PYSPARK_DRIVER_PYTHON="jupyter"
export PYSPARK_DRIVER_PYTHON_OPTS="notebook"
export PYSPARK_PYTHON=python3
export PATH=$SPARK_HOME:$PATH:~/.local/bin:$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$HIVE_HOME/bin


#Hadoop Related Options
export HADOOP_HOME=/home/abhijeet/Downloads/hadoop-3.2.1
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_OPTS"-Djava.library.path=$HADOOP_HOME/lib/nativ"





Edit core-site.xml File>>>
<configuration>
<property>
  <name>hadoop.tmp.dir</name>
  <value>/home/abhijeet/tmpdata</value>
</property>
<property>
  <name>fs.default.name</name>
  <value>hdfs://127.0.0.1:9000</value>
</property>
</configuration>


Edit hdfs-site.xml File>>
<configuration>
<property>
  <name>dfs.data.dir</name>
  <value>/home/abhijeet/dfsdata/namenode</value>
</property>
<property>
  <name>dfs.data.dir</name>
  <value>/home/abhijeet/dfsdata/datanode</value>
</property>
<property>
  <name>dfs.replication</name>
  <value>1</value>
</property>
</configuration>

Edit mapred-site.xml File
<configuration> 
<property> 
  <name>mapreduce.framework.name</name> 
  <value>yarn</value> 
</property> 
</configuration>


Use the following command to access the mapred-site.xml file and define MapReduce values:

sudo nano $HADOOP_HOME/etc/hadoop/mapred-site.xml



<configuration> 
<property> 
  <name>mapreduce.framework.name</name> 
  <value>yarn</value> 
</property> 
</configuration>
The mapred configuration file content for a single node Hadoop cluster.

Edit yarn-site.xml File
<configuration>
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>
<property>
  <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
  <value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>127.0.0.1</value>
</property>
<property>
  <name>yarn.acl.enable</name>
  <value>0</value>
</property>
<property>
  <name>yarn.nodemanager.env-whitelist</name>   
  <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PERPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
</property>
</configuration>
